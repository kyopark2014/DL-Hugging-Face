# Deep Learning: Hugging-Face

## Transformer

- 인공신경망 알고리즘중 하나
- 2017년 구글이 발표한 논문인 "Attention is all you need"에서 나온 모델
- Self-Attention 방식을 사용: 문장 전체를 병렬구조로 번역할 뿐만 아니라, 멀리 있는 단어까지도 연관성을 만들어 유사성을 높였으며, RNN의 한계를 극복
- GPT-3, BERT


## Hugging-Face


- 다양한 트랜스포머 모델(transformer.models)과 학습 스크립트(transformer.Trainer)를 제공하는 모듈

### Estimator

![image](https://user-images.githubusercontent.com/52392004/218343273-c2d6ff2b-0ed0-498d-b561-c6c4df9eb88c.png)


## Inference 

![image](https://user-images.githubusercontent.com/52392004/204080227-0a741ead-1b1d-49ea-a51c-18d08b449e53.png)


## Reference 

[Github - Hugging Face](https://github.com/huggingface/transformers/blob/main/README_ko.md)

[[DL] Hugging Face란?](https://wooono.tistory.com/413)

[Introduction to Hugging Face on Amazon SageMaker | Amazon Web Services](https://www.youtube.com/watch?v=80ix-IyNnQI)

[Hugging Face on Amazon SageMake](https://aws.amazon.com/machine-learning/hugging-face/?sc_channel=EL&sc_campaign=Demo_2022_vid&sc_medium=YouTube&sc_content=&sc_detail=MACHINE_LEARNING&sc_country=US)
